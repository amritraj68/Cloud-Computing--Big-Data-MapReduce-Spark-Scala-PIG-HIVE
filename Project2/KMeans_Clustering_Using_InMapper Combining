In this project, you are asked to improve the performance of your code developed in Project 1 using in-mapper combining. 

You can compile KMeans.java on Comet using:


run kmeans.build
and you can run KMeans.java in standalone mode over a small dataset using:

sbatch kmeans.local.run

The new centroids generated by your program will be in the directory output and must be similar to solution-small.txt. You should develop and run your programs in standalone mode until you get the correct result. After you make sure that your program runs correctly in standalone mode, you run it in distributed mode using:


sbatch kmeans.distr.run
This will calculate kmeans on the large dataset points-large.txt and will write the result in the directory output-distr. 

Pseudo-Code : 

You should modify your project2/src/main/java/KMeans.java only, which is a copy of your KMeans.java file from Project 1. For this project though you will also use a hash table table, which for each centroid c (the hash table key), it holds the object Avg(sumX,sumY,count), where sumX and sumY are partial sums, and count is a partial count, so that the new centroid for c is at (sumX/count,sumY/count):

class Point {
    public double x;
    public double y;
}

class Avg {
    public double sumX;
    public double sumY;
    public long count;
}

Vector[Point] centroids;
Hashtable[Point,Avg] table;

mapper setup:
  read centroids from the distributed cache
  initialize table

mapper cleanup:
  for each key c in table
      emit(c,table[c])

map ( key, line ):
  Point p = new Point()
  read 2 double numbers from the line (x and y) and store them in p
  find the closest centroid c to p
  if table[c] is empty
     then table[c] = new Avg(x,y,1)
     else table[c] = new Avg(table[c].sumX+x,table[c].sumY+y,table[c].count+1)

reduce ( c, avgs ):
  count = 0
  sx = sy = 0.0
  for a in avgs
      sx += a.sumX
      sy += a.sumY
      count += a.count
  c.x = sx/count
  c.y = sy/count
  emit(c,null)
